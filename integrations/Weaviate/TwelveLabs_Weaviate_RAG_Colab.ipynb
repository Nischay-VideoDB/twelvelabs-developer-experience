{"cells":[{"cell_type":"markdown","metadata":{"id":"MFVINhYtd8hO"},"source":["# Twelve Labs Video RAG with Weaviate"]},{"cell_type":"markdown","metadata":{"id":"auIAI4xId8hS"},"source":["## Set Up Our Environment\n","\n","### Install Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DnsNWwDmd8hT"},"outputs":[],"source":["!python -m pip install -U -q twelvelabs\n","!python -m pip install -U -q weaviate-client"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsoWZbBEd8hU"},"outputs":[],"source":["!python -m pip install -q av\n","!python -m pip install --upgrade -q accelerate\n","!python -m pip install -U bitsandbytes\n","!python -m pip install git+https://github.com/huggingface/transformers.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCzcHj7Dd8hV"},"outputs":[],"source":["!python -m pip install pillow\n","!python -m pip install sentencepiece\n","!python -m pip install matplotlib"]},{"cell_type":"markdown","metadata":{"id":"XczFDAzwd8hV"},"source":["### Set Up Twelve Labs and Weaviate SDKs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnRnWMbjd8hV"},"outputs":[],"source":["import os\n","from google.colab import userdata\n","\n","TL_API_KEY=userdata.get('TL_API_KEY')\n","\n","weaviate_url = userdata.get(\"WEAVIATE_URL\")\n","weaviate_api_key = userdata.get(\"WEAVIATE_API_KEY\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_8SmISAd8hW"},"outputs":[],"source":["from twelvelabs import TwelveLabs\n","\n","# Initialize the Twelve Labs client\n","twelve_labs_client = TwelveLabs(api_key=TL_API_KEY)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDCtbXq3d8hW"},"outputs":[],"source":["import weaviate\n","from weaviate.classes.init import Auth\n","\n","# Connect to Weaviate Cloud\n","weaviate_client = weaviate.connect_to_weaviate_cloud(\n","    cluster_url=weaviate_url,\n","    auth_credentials=Auth.api_key(weaviate_api_key),\n",")\n","\n","# Get or create collection\n","try:\n","    collection = weaviate_client.collections.get(\"Video_Embeddings\")\n","except:\n","    collection = weaviate_client.collections.create(name=\"Video_Embeddings\")"]},{"cell_type":"markdown","source":["### Setting Up Our Video Data"],"metadata":{"id":"9A7VvcGogKDN"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","base_folder_path = \"/content/drive/MyDrive/TwelveLabs-Weaviate\"\n","raw_video_dir = base_folder_path + \"/sports_videos\"\n","\n","upscaled_video_dir = base_folder_path + \"/upscaled_videos/\"\n","video_segments_dir = base_folder_path + \"/video_segments/\""],"metadata":{"id":"LrOLBpTBgHVV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"35_SHLgud8hX"},"source":["### Setting Up Our Video Data\n","Some of our videos are too low resolution to use in the embedding engine, so we will double their their resolution with `upscale_video`.\n","\n","read_video_pyav comes directly from the [LLaVa-NeXT-Video collab notebook](https://colab.research.google.com/drive/1CZggLHrjxMReG-FNOmqSOdi4z7NPq6SO?usp=sharing#scrollTo=hqpPqDKuQUTq) and it formats videos in the correct numpy representation for inference.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6HVGDT3d8hX"},"outputs":[],"source":["import av\n","import numpy as np\n","\n","def upscale_video(input_file, output_file, target_width=1280, target_height=720):\n","    input_container = av.open(input_file)\n","    output_container = av.open(output_file, mode='w')\n","\n","    input_stream = input_container.streams.video[0]\n","    output_stream = output_container.add_stream('libx264', rate=input_stream.average_rate)\n","    output_stream.width = target_width\n","    output_stream.height = target_height\n","    output_stream.pix_fmt = 'yuv420p'\n","\n","    for frame in input_container.decode(input_stream):\n","        frame = frame.reformat(width=target_width, height=target_height)\n","        packet = output_stream.encode(frame)\n","        output_container.mux(packet)\n","\n","    # Flush the encoder\n","    packet = output_stream.encode(None)\n","    output_container.mux(packet)\n","\n","    # Close the containers\n","    input_container.close()\n","    output_container.close()\n","\n","def read_video_pyav(container, indices):\n","    '''\n","    Decode the video with PyAV decoder.\n","\n","    Args:\n","        container (av.container.input.InputContainer): PyAV container.\n","        indices (List[int]): List of frame indices to decode.\n","\n","    Returns:\n","        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n","    '''\n","    frames = []\n","    container.seek(0)\n","    start_index = indices[0]\n","    end_index = indices[-1]\n","    for i, frame in enumerate(container.decode(video=0)):\n","        if i > end_index:\n","            break\n","        if i >= start_index and i in indices:\n","            frames.append(frame)\n","    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n"]},{"cell_type":"markdown","metadata":{"id":"RZ-okL7Yd8hY"},"source":["Here we upscale all of our videos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSnzld9Gd8hY"},"outputs":[],"source":["# Create output directory if it doesn't exist\n","if not os.path.exists(upscaled_video_dir):\n","    os.makedirs(upscaled_video_dir)\n","\n","# Iterate over all files in the raw video directory\n","for filename in os.listdir(raw_video_dir):\n","\n","    # Check if the file is a video file\n","    if filename.endswith(\".mp4\"):\n","        print(filename)\n","        # Get the file name without extension\n","        input_file_no_ext = os.path.splitext(filename)[0]\n","        # Define the output file name\n","        output_file = f\"{input_file_no_ext}_480.mp4\"\n","        if output_file in os.listdir(upscaled_video_dir):\n","            continue\n","        # Define the full path for the input and output files\n","        input_file_path = os.path.join(raw_video_dir, filename)\n","        output_file_path = os.path.join(upscaled_video_dir, output_file)\n","        # Upscale the video\n","        upscale_video(input_file_path, output_file_path)\n"]},{"cell_type":"markdown","metadata":{"id":"MW0ALq1ud8hZ"},"source":["## Compare Pegasus and LLaVA-NeXT-Video on a Single Video\n","\n","We will start by comparing Pegausus and LLaVA-NeXT-Video on generating insights from a single video\n","\n","### Using Pegasus to Chat with our Video\n","\n","To chat with our video, we first need to have Pegasus index it.\n","\n","We will create an index named `sports_videos` and then upload our video to this index to be indexed before chatting with it. We only need to do this once per video.\n","\n","In more complex workflows with multiple videos, we can upload all of can be done way ahead of time to reduce overhead and speed up the end-to-end workflow.\n"]},{"cell_type":"markdown","metadata":{"id":"_7lyvuamd8hZ"},"source":["First we create the index."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJcpORG6d8hZ"},"outputs":[],"source":["models = [\n","        {\n","            \"name\": \"pegasus1.2\",\n","            \"options\": [\"visual\"]\n","        }\n","    ]\n","\n","index_name = \"sports_videos\"\n","indices_list = twelve_labs_client.index.list(name=index_name)\n","\n","if len(indices_list) == 0:\n","    index = twelve_labs_client.index.create(\n","        name=index_name,\n","        models=models\n","\n","    )\n","    print(f\"A new index has been created: id={index.id} name={index.name} models={index.models}\")\n","else:\n","    index = indices_list[0]\n","    print(f\"Index already exists: id={index.id} name={index.name} models={index.models}\")"]},{"cell_type":"markdown","metadata":{"id":"KD94f1gQd8ha"},"source":["Then we create a funciton to upload our video to be indexed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhctXINcd8ha"},"outputs":[],"source":["# Monitor the status of the video task\n","def on_task_update(task):\n","    print(f\"  Status={task.status}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQC7ZI2vd8ha"},"outputs":[],"source":["def upload_video_to_twelve_labs_pegasus(video_path):\n","    print(video_path)\n","    task = twelve_labs_client.task.create(\n","        index_id=index.id,\n","        file = video_path\n","    )\n","    print(f\"Task created: id={task.id} status={task.status}\")\n","\n","    task.wait_for_done(sleep_interval=5, callback=on_task_update)\n","\n","    if task.status != \"ready\":\n","      raise RuntimeError(f\"Indexing failed with status {task.status}\")\n","    print(f\"The unique identifer of your video is {task.video_id}.\")\n","    return task.video_id"]},{"cell_type":"markdown","metadata":{"id":"HDPz6E9Rd8hb"},"source":["Next, we'll upload our video."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfvpP0kod8hb"},"outputs":[],"source":["# Define the video file path\n","single_video_file = upscaled_video_dir + \"football_480.mp4\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WDC3rLagd8hb"},"outputs":[],"source":["single_video_id = upload_video_to_twelve_labs_pegasus(single_video_file)"]},{"cell_type":"markdown","metadata":{"id":"J6yJk4w1d8hb"},"source":["Finally we'll query it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aCilV_m1d8hc"},"outputs":[],"source":["single_video_query = \"What is going on in this video? Please be concise.\"\n","\n","res = twelve_labs_client.generate.text(\n","  video_id=single_video_id,\n","  prompt=single_video_query\n",")\n","print(f\"{res.data}\")"]},{"cell_type":"markdown","metadata":{"id":"3Fgvpsied8hc"},"source":["Here is Pegasus' response:\n","```\n","The video showcases a pivotal moment in a football game between the New York Giants and the New England Patriots. Eli Manning, the Giants' quarterback, throws a pass that David Tyree catches spectacularly by pinning the ball against his helmet as he falls out of bounds. Multiple angles replay the catch, emphasizing its difficulty and precision. Tyree briefly celebrates after the play, and the video ends with him and other players walking off the field.\n","```\n","\n","From the above response, we can see that Pegagus 1.2 can coherently resopnd to the question. Now, lets check and see if we can get a similar response from the Open Source model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3r6z3pc0d8hc"},"outputs":[],"source":["\n","res = twelve_labs_client.generate.text(\n","  video_id=single_video_id,\n","  prompt=\"What game is this?\"\n",")\n","print(f\"{res.data}\")"]},{"cell_type":"markdown","metadata":{"id":"kruETmtkd8hc"},"source":["### Using LLaVa-NeXT-Video to Chat with our Video\n","For the Open Source model, we will need to setup up a video sampling for the model to consume and load the model from the Hugging Face Hub, format the input for inference, and then run the model on our inputs. We will modify the [LLaVa-NeXT-Video Sampling code](https://colab.research.google.com/drive/1CZggLHrjxMReG-FNOmqSOdi4z7NPq6SO?usp=sharing#scrollTo=hqpPqDKuQUTq) to get a uniform sample of 40 frames for each video.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7oYVqjN9d8hd"},"outputs":[],"source":["def sample_video(video_path, num_samples=8):\n","    container = av.open(video_path)\n","\n","    # sample uniformly num_samples frames from the video\n","    total_frames = container.streams.video[0].frames\n","    indices = np.arange(0, total_frames, total_frames / num_samples).astype(int)\n","\n","    sampled_frames = read_video_pyav(container, indices)\n","\n","    return sampled_frames"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1SxwnP_d8hd"},"outputs":[],"source":["sampled_video = sample_video(single_video_file, num_samples=40)"]},{"cell_type":"markdown","metadata":{"id":"DDk_LSuId8hd"},"source":["Here we'll set up our LLaVa-NeXT-Video model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_yAKwo0d8he"},"outputs":[],"source":["from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n","import torch\n","\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","llava_next_processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n","llava_next_model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n","    \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n","    quantization_config=quantization_config,\n","    device_map='auto'\n",")"]},{"cell_type":"markdown","metadata":{"id":"_79r2P6xd8he"},"source":["Next, we'll create a function to query our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4cT4BRxd8he"},"outputs":[],"source":["def query_llava_next(query,model,processor,sampled_video):\n","\n","    # Each \"content\" is a list of dicts and you can add image/video/text modalities\n","    conversation = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"text\", \"text\": query},\n","                {\"type\": \"video\"},\n","                ],\n","        },\n","    ]\n","\n","    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n","    # prompt_len = len(prompt)\n","\n","    inputs = processor([prompt], videos=[sampled_video], padding=True, return_tensors=\"pt\").to(model.device)\n","\n","    generate_kwargs = {\"max_new_tokens\": 100, \"do_sample\": True, \"top_p\": 0.9}\n","\n","    output = model.generate(**inputs, **generate_kwargs)\n","    generated_text = processor.batch_decode(output, skip_special_tokens=True)\n","\n","    return generated_text[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OrbRKbGd8hf"},"outputs":[],"source":["llava_next_result = query_llava_next(single_video_query,llava_next_model,llava_next_processor,sampled_video)\n","print(llava_next_result)"]},{"cell_type":"markdown","metadata":{"id":"hRMQXIazd8hf"},"source":["### Output:\n","\n","Here is LLaVa-NeXT-Video's ouput:\n","\n","```\n","What is happening in this video? Be concise ASSISTANT: The video shows a football game in progress, with various players on the field. It appears to be the Super Bowl III between the New York Giants and the New England Patriots, judging by the jersey numbers and the old-fashioned helmets worn by some players. One player is in mid-action, grabbing the ball and getting tackled by another player, while a referee is signaling a first down. There are also coaches and other game\n","```\n","\n","While this model does recognize that there is a football game happening between the Giants and the Patriots, it tends to hallucinate other facts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcPul4Ncd8hg"},"outputs":[],"source":["llava_next_result = query_llava_next(\"what game is this?\",llava_next_model,llava_next_processor,sampled_video)\n","print(llava_next_result)"]},{"cell_type":"markdown","metadata":{"id":"awR8WtgGd8hg"},"source":["# RAG for Segment-Level Queries on a Single Video\n","\n","We see that Pegasus is the clear winner on time and accuracy for this query when querying the entire video.\n","\n","The open source model would likely perform better if we could constrict the video in question to a smaller segment. We can do this by creating queries that only need a subset of the video, and using RAG to get the relevant subset.\n","\n","This is where the Marengo model will come in. We can use it to create embeddings for each segment of the video, and then use RAG to get the most relevant segment based on our queries.\n","\n","We will start by creating embeddings for each segment of the video."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpoNLY4Md8hg"},"outputs":[],"source":["# Define the video segment length\n","segment_length = 10"]},{"cell_type":"markdown","metadata":{"id":"oG9JoOqXd8hh"},"source":["### Using Marengo to Create Full Video and Video Clip Embeddings\n","\n","Marengo allows us to retrieve embeddings for the entire video and for clips at a set clip length all in one call."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnhyFz_od8hi"},"outputs":[],"source":["task = twelve_labs_client.embed.task.create(\n","    model_name=\"Marengo-retrieval-2.7\",\n","    video_file=single_video_file,\n","    video_clip_length=segment_length,\n","    video_embedding_scopes=[\"clip\", \"video\"]\n",")\n","print(\n","    f\"Created task: id={task.id} model_name={task.model_name} status={task.status}\"\n",")\n","\n","# Monitor the status of the video embedding task\n","status = task.wait_for_done(\n","    sleep_interval=2,\n","    callback=on_task_update\n",")\n","print(f\"Embedding done: {status}\")"]},{"cell_type":"markdown","metadata":{"id":"t-iB6Fc0d8hi"},"source":["We'll save the task ID for use later when uploading our embeddings to Weaviate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2MsfffTd8hj"},"outputs":[],"source":["# single_video_task_id = task.id\n","single_video_task_id = \"67cfa68dccd453a4969c8785\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d44Zyo-od8hj"},"outputs":[],"source":["marengo_task_ids = {}\n","single_video_file_name = single_video_file.split(\"/\")[-1]\n","marengo_task_ids[single_video_file_name] = single_video_task_id"]},{"cell_type":"markdown","metadata":{"id":"48pVTIKMd8hj"},"source":["### Prepare Video Segments for RAG"]},{"cell_type":"markdown","metadata":{"id":"Yt3I6IT9d8hj"},"source":["Next, we will split this video up into segments that mirror the timestamps for each embedding. This lets us later submit _only_ this video chunk to our model for a RAG use case"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Go3p8hbYd8hj"},"outputs":[],"source":["import os\n","import subprocess\n","import json\n","\n","def split_video(input_path, output_dir, segment_duration=10):\n","    \"\"\"\n","    Split a video into segments of the specified duration.\n","    Regular segments will be exactly segment_duration seconds.\n","    The last segment will be at least 5 seconds long, potentially overlapping\n","    with the previous segment if needed.\n","\n","    Args:\n","        input_path: Path to the input video file\n","        output_dir: Directory to save the output segments\n","        segment_duration: Duration of each segment in seconds (default: 10)\n","    \"\"\"\n","\n","    # Minimum length for the last segment\n","    min_last_segment_len = 5\n","\n","    # Create output directory if it doesn't exist\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Get base filename without extension\n","    base_name = os.path.splitext(os.path.basename(input_path))[0]\n","\n","    # Get video duration using ffprobe\n","    probe_cmd = [\n","        \"ffprobe\", \"-v\", \"quiet\", \"-print_format\", \"json\",\n","        \"-show_format\", input_path\n","    ]\n","\n","    try:\n","        probe_result = subprocess.run(probe_cmd, capture_output=True, text=True, check=True)\n","        video_info = json.loads(probe_result.stdout)\n","        duration = float(video_info[\"format\"][\"duration\"])\n","    except Exception as e:\n","        print(f\"Error getting video duration: {e}\")\n","        return 0\n","\n","    # Calculate number of full segments\n","    num_full_segments = int(duration / segment_duration)\n","\n","    # Calculate remaining duration\n","    remaining_duration = duration - (num_full_segments * segment_duration)\n","\n","    # Determine total number of segments and if we need to adjust the last segment\n","    if remaining_duration > 0:\n","        if remaining_duration < min_last_segment_len:\n","            # Last segment would be too short, so we'll adjust its start time\n","            num_segments = num_full_segments + 1\n","            needs_adjustment = True\n","        else:\n","            # Last segment is already long enough\n","            num_segments = num_full_segments + 1\n","            needs_adjustment = False\n","    else:\n","        # No remaining duration, all segments are complete\n","        num_segments = num_full_segments\n","        needs_adjustment = False\n","\n","    print(f\"Video {base_name} is {duration:.2f} seconds long\")\n","    print(f\"Creating {num_segments} segments\")\n","\n","    # Create each segment\n","    for i in range(num_segments):\n","        # For regular segments, start at the segment boundary\n","        if i < num_full_segments:\n","            start_time = i * segment_duration\n","            actual_duration = segment_duration\n","        else:\n","            # This is the last segment\n","            if needs_adjustment:\n","                # Start earlier to ensure it's at least min_last_segment_len seconds\n","                start_time = duration - min_last_segment_len\n","                actual_duration = min_last_segment_len\n","            else:\n","                # Last segment is already long enough\n","                start_time = i * segment_duration\n","                actual_duration = remaining_duration\n","\n","        output_path = os.path.join(output_dir, f\"{base_name}_segment_{i:03d}.mp4\")\n","\n","        # For all segments, use copy mode for speed\n","        cmd = [\n","            \"ffmpeg\", \"-y\",\n","            \"-ss\", str(start_time),\n","            \"-i\", input_path,\n","            \"-t\", str(actual_duration),\n","            \"-c:v\", \"copy\",\n","            \"-c:a\", \"copy\",\n","            output_path\n","        ]\n","\n","        result = subprocess.run(cmd, capture_output=True, text=True)\n","\n","        if result.returncode != 0:\n","            print(f\"Error creating segment {i+1}: {result.stderr[:100]}...\")\n","        else:\n","            end_time = start_time + actual_duration\n","            if i == num_segments - 1 and needs_adjustment:\n","                print(f\"Created segment {i+1}/{num_segments}: {start_time:.1f}s to {end_time:.1f}s (adjusted to ensure at least {min_last_segment_len}s)\")\n","            else:\n","                print(f\"Created segment {i+1}/{num_segments}: {start_time:.1f}s to {end_time:.1f}s\")\n","\n","    print(f\"Successfully split {base_name} into {num_segments} segments\")\n","    return num_segments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2i8dc3Ud8hk"},"outputs":[],"source":["split_video(single_video_file, video_segments_dir,segment_length)"]},{"cell_type":"markdown","metadata":{"id":"51gKpYQtd8hl"},"source":["Next, we'll upload the video segments to Pegaus to get their video ids. We will upload these to Weaviate along with the embeddings, so we can easily chat with the returned video. This is a great way to speed up results when you have videos that users will chat with."]},{"cell_type":"markdown","metadata":{"id":"nk96gPLFd8hl"},"source":["Here we'll create and populate a dictionary mapping file names with pegasus video IDs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwX1j1pid8hl"},"outputs":[],"source":["pegasus_video_ids = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1Vj8IWWd8hl"},"outputs":[],"source":["segment_video_files = [f for f in os.listdir(video_segments_dir) if f.endswith(('.mp4'))]\n","\n","# Process each video\n","for segment_video_file in segment_video_files:\n","    if segment_video_file in pegasus_video_ids:\n","        print(\"skip file\",segment_video_file)\n","        continue\n","    print(\"processing file\",segment_video_file)\n","    try:\n","        video_id = upload_video_to_twelve_labs_pegasus(video_segments_dir+segment_video_file)\n","        pegasus_video_ids[segment_video_file] = video_id\n","\n","    except Exception as e:\n","        print(f\"Error processing {segment_video_file}: {str(e)}\")\n","        continue"]},{"cell_type":"markdown","metadata":{"id":"SJKxHrfHd8hm"},"source":["We'll also add the video ID for the full video that we retrieved earlier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSAexN7ed8hm"},"outputs":[],"source":["fname = single_video_file.split(\"/\")[-1]\n","pegasus_video_ids[fname] = single_video_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJbjinUGd8hm"},"outputs":[],"source":["print(pegasus_video_ids)"]},{"cell_type":"markdown","metadata":{"id":"7sG_x9-Dd8hm"},"source":["We'll also sample all of our videos for use with the LLaVa-NeXT-Video model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZiYqo4wd8hm"},"outputs":[],"source":["sampled_video_files = {}\n","\n","for video_file in os.listdir(video_segments_dir):\n","    print(video_file)\n","    sampled_video = sample_video(video_segments_dir + video_file,num_samples=40)\n","    sampled_video_files[video_file] = sampled_video\n","\n","for video_file in os.listdir(upscaled_video_dir):\n","    print(video_file)\n","    sampled_video = sample_video(upscaled_video_dir + video_file,num_samples=40)\n","    sampled_video_files[video_file] = sampled_video"]},{"cell_type":"markdown","metadata":{"id":"rSpyuCGJd8hn"},"source":["### Uploading Embeddings to Weaviate\n","\n","Now we'll create a function to prepare our data to be uploaded to Weaviate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUvytwV3d8hn"},"outputs":[],"source":["def prepare_marengo_embeddings_for_weaviate(marengo_task_ids,pegasus_video_ids):\n","\n","    # Prepare data for Weaviate upload\n","    records = []\n","    vectors = []\n","\n","    for video_file_name in marengo_task_ids.keys():\n","\n","\n","        marengo_task_id = marengo_task_ids[video_file_name]\n","\n","        # Retreive marengo full video and clip embeddings\n","        marengo_embeddings_result = twelve_labs_client.embed.task.retrieve(marengo_task_id)\n","\n","\n","        #track segment number to match with fiel\n","        segment_number = 0\n","\n","        for segment in marengo_embeddings_result.video_embedding.segments:\n","            # Determine if this is a video or clip segment\n","            is_video = segment.embedding_scope == \"video\"\n","\n","\n","            #Update the file name if segment\n","            updated_file_name = video_file_name\n","            if not is_video:\n","                updated_file_name = updated_file_name.replace(\".mp4\",f\"_segment_{segment_number:03d}.mp4\")\n","                segment_number += 1\n","\n","            video_name = video_file_name.replace(\".mp4\",\"\")\n","\n","            pegasus_video_id = None\n","            if updated_file_name in pegasus_video_ids:\n","                pegasus_video_id = pegasus_video_ids[updated_file_name]\n","\n","            record = {\n","                'video_name':video_name,\n","                'segment_number': 0 if is_video else segment_number,\n","                'video_file': updated_file_name,\n","                'start_time': getattr(segment, 'start_offset_sec', 0),\n","                'end_time': getattr(segment, 'end_offset_sec', 0),\n","                'type': 'video' if is_video else 'clip',\n","                'task_id': marengo_task_id,\n","                'pegasus_video_id': pegasus_video_id\n","            }\n","\n","            # Get the embedding vector\n","            embedding_vector = [float(x) for x in segment.embeddings_float]\n","\n","            # Add to our lists\n","            records.append(record)\n","            vectors.append(embedding_vector)\n","\n","    # Print summary\n","    print(f\"Prepared {len(records)} segments for upload to Weaviate\")\n","    print(f\"- Video embeddings: {sum(1 for r in records if r['type'] == 'video')}\")\n","    print(f\"- Clip embeddings: {sum(1 for r in records if r['type'] == 'clip')}\")\n","\n","    return records, vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4fXIl_jd8hn"},"outputs":[],"source":["records, vectors = prepare_marengo_embeddings_for_weaviate(marengo_task_ids,pegasus_video_ids)"]},{"cell_type":"markdown","metadata":{"id":"KzkyrVRsd8ho"},"source":["Now, we'll upload the data to our collection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTZ4evhkd8ho"},"outputs":[],"source":["with collection.batch.dynamic() as batch:\n","    for i, record in enumerate(records):\n","        batch.add_object(\n","            properties=record,\n","            vector=vectors[i]\n","        )\n","\n","print(f\"Added {len(records)} embeddings to Weaviate\")"]},{"cell_type":"markdown","metadata":{"id":"SlozWCVLd8ho"},"source":["### Testing the Vector Search\n","Now that we have everything in the collection, we can test and see that it properly returns the correct sample `'video_name':5.0`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nruZZhLCd8ho"},"outputs":[],"source":["from weaviate.classes.query import MetadataQuery, Filter\n","\n","# Use a specific vector for the query\n","query_vector = vectors[5]\n","\n","# Perform vector search\n","response = collection.query.near_vector(\n","    near_vector=query_vector,\n","    limit=1,  # Increased limit to get more results\n","    return_metadata=MetadataQuery(distance=True),\n",")\n","\n","print(f\"Found {len(response.objects)} results for vector search\")\n","for obj in response.objects:\n","    print(f\"Video: {obj.properties['video_file']}, Type: {obj.properties['type']}\")\n","    if 'segment_id' in obj.properties:\n","        print(f\"Segment: {obj.properties['segment_id']}\")\n","    if 'text' in obj.properties and obj.properties['text']:\n","        print(f\"Text: {obj.properties['text']}\")\n","    print(f\"Distance: {obj.metadata.distance}\")\n","    print(\"-\" * 50)"]},{"cell_type":"markdown","metadata":{"id":"NRJYVc_Bd8ho"},"source":["### Querying our Vector Database with Text Embeddings\n","\n","To query the database, first we'll embed our text query with Marengo's text embedding feature. Then we will query the Weaviate database for the clip embedding that best matches our question embeddings. We will then use the pegasus video ID to ask our question for that clip."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNxJllV9d8hp"},"outputs":[],"source":["sample_question = \"What technique did David Tyree use to catch the ball?\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgM2PJ13d8hp"},"outputs":[],"source":["embedding = twelve_labs_client.embed.create(\n","    model_name=\"Marengo-retrieval-2.7\",\n","    text=sample_question,\n","    text_truncate=\"start\",\n",")\n","\n","query_vector = embedding.text_embedding.segments[0].embeddings_float\n","\n","response = collection.query.near_vector(\n","    near_vector=query_vector,\n","    limit=1,\n","    return_metadata=MetadataQuery(distance=True),\n","    filters=(Filter.by_property(\"type\").equal(\"clip\"))\n",")\n","\n","video_file = response.objects[0].properties.get(\"video_file\")\n","pegasus_video_id = response.objects[0].properties.get(\"pegasus_video_id\")\n","print(video_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AK5OQ8ZGd8hp"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib import animation\n","from IPython.display import HTML\n","\n","video = sampled_video_files[video_file]\n","\n","fig = plt.figure()\n","im = plt.imshow(video[0,:,:,:])\n","\n","plt.close() # this is required to not display the generated image\n","\n","def init():\n","    im.set_data(video[0,:,:,:])\n","\n","def animate(i):\n","    im.set_data(video[i,:,:,:])\n","    return im\n","\n","anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n","                               interval=100)\n","HTML(anim.to_html5_video())"]},{"cell_type":"markdown","metadata":{"id":"zyxy9W4Vd8hp"},"source":["### Chatting with our Video Segment: Pegasus vs LLaVa-NeTX-Video"]},{"cell_type":"markdown","metadata":{"id":"azWiobvXd8hq"},"source":["Pegasus:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYVznvRUd8hq"},"outputs":[],"source":["print(sample_question)\n","\n","res = twelve_labs_client.generate.text(\n","  video_id=pegasus_video_id,\n","  prompt=sample_question\n",")\n","print(f\"{res.data}\")"]},{"cell_type":"markdown","metadata":{"id":"jOGv0U1Sd8hq"},"source":["LLaVa-NeXT-Video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulfMwW-6d8hq"},"outputs":[],"source":["sampled_video = sampled_video_files[video_file]\n","generated_text = query_llava_next(sample_question,llava_next_model,llava_next_processor,sampled_video)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVyRF10md8hr"},"outputs":[],"source":["print(generated_text)"]},{"cell_type":"markdown","metadata":{"id":"kDRL9nvbd8hr"},"source":["## Multi Video RAG with Marengo, Weaviate, and Pegasus\n","\n","Now that we know how Marengo embeddings perform on individual clips from a single video, we will show how to use embeddings across mutiple videos for a more realistic RAG use case"]},{"cell_type":"markdown","metadata":{"id":"-E5r-_C5d8hr"},"source":["### Get Marengo Embeddings for All Videos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_LAA7hwd8hr"},"outputs":[],"source":["for video_file_name in os.listdir(upscaled_video_dir):\n","\n","    if video_file_name in marengo_task_ids:\n","        print(f\"skipping {video_file_name} because embeddings already exist\")\n","        continue\n","\n","    print(f\"processing {video_file_name}\")\n","\n","    file_path = os.path.join(upscaled_video_dir, video_file_name)\n","\n","    task = twelve_labs_client.embed.task.create(\n","        model_name=\"Marengo-retrieval-2.7\",\n","        video_file=file_path,\n","        video_clip_length=segment_length,\n","        video_embedding_scopes=[\"clip\", \"video\"]\n","    )\n","    print(\n","        f\"Created task: id={task.id} model_name={task.model_name} status={task.status}\"\n","    )\n","\n","    # Monitor the status of the video embedding task\n","    status = task.wait_for_done(\n","        sleep_interval=2,\n","        callback=on_task_update\n","    )\n","    print(f\"Embedding done: {status}\")\n","\n","    marengo_task_ids[video_file_name] = task.id\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7AwCUrXd8hs"},"outputs":[],"source":["print(marengo_task_ids)"]},{"cell_type":"markdown","metadata":{"id":"eGpyTGJcd8hs"},"source":["### Split our Remaining Videos into Segments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDTxQqH_d8hs"},"outputs":[],"source":["# Create output folder if it doesn't exist\n","os.makedirs(upscaled_video_dir, exist_ok=True)\n","\n","# Get all video files\n","video_files = [f for f in os.listdir(upscaled_video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n","\n","# Process each video\n","for video_file in video_files:\n","    split_video(upscaled_video_dir + video_file,video_segments_dir,segment_length)"]},{"cell_type":"markdown","metadata":{"id":"wPdfBbMAd8hs"},"source":["### Get Pegasus Video IDs for All Videos and their Segments"]},{"cell_type":"markdown","metadata":{"id":"Uz6D9sA5d8hs"},"source":["Finally, we will upload the full videos and their segments to Pegasus so we can chat with them. We will paralellize this task to speed it up."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPx-kRcod8ht"},"outputs":[],"source":["import concurrent.futures\n","import os\n","from tqdm import tqdm  # Use standard tqdm instead of tqdm.notebook\n","\n","def process_video(video_path):\n","    video_file_name = video_path.split(\"/\")[-1]\n","    try:\n","        video_id = upload_video_to_twelve_labs_pegasus(video_path)\n","        return video_file_name, video_id\n","    except Exception as e:\n","        print(f\"Error processing {video_file_name}: {str(e)}\")\n","        return video_file_name, None\n","\n","# Filter out videos that are already processed\n","segment_video_files = [ video_segments_dir + f for f in os.listdir(video_segments_dir) if f.endswith('.mp4')]\n","full_video_files = [ upscaled_video_dir + f for f in os.listdir(upscaled_video_dir) if f.endswith('.mp4')]\n","all_video_files = segment_video_files + full_video_files\n","\n","videos_to_process = [f for f in all_video_files if f.split(\"/\")[-1] not in pegasus_video_ids]\n","\n","print(f\"Processing {len(videos_to_process)} videos in parallel...\")\n","\n","# Use ThreadPoolExecutor for I/O-bound operations like API calls\n","with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n","    # Submit all tasks and create a dictionary mapping futures to their video files\n","    future_to_video = {executor.submit(process_video, video_path): video_path for video_path in videos_to_process}\n","\n","    # Process results as they complete with a progress bar\n","    for future in tqdm(concurrent.futures.as_completed(future_to_video), total=len(videos_to_process)):\n","        video_file_name, video_id = future.result()\n","        if video_id:\n","            pegasus_video_ids[video_file_name] = video_id\n","\n","print(\"All videos processed!\")\n","print(f\"Successfully processed {len([v for v in pegasus_video_ids.values() if v is not None])} videos\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5KjXul8d8hu"},"outputs":[],"source":["print(pegasus_video_ids)"]},{"cell_type":"markdown","metadata":{"id":"-wjpSHr3d8hu"},"source":["### Upload Data to Weaviate\n","First we'll prepare our data to be uploaded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKpdkZG-d8hu"},"outputs":[],"source":["records, vectors = prepare_marengo_embeddings_for_weaviate(marengo_task_ids,pegasus_video_ids)"]},{"cell_type":"markdown","metadata":{"id":"EWXkgN7qd8hv"},"source":["Then, we will upload it to our collection."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQxrI_tHd8hv"},"outputs":[],"source":["with collection.batch.dynamic() as batch:\n","    for i, record in enumerate(records):\n","        if record[\"pegasus_video_id\"] is None:\n","            continue\n","        batch.add_object(\n","            properties=record,\n","            vector=vectors[i]\n","        )\n","\n","print(f\"Added {len(records)} embeddings to Weaviate\")"]},{"cell_type":"markdown","metadata":{"id":"C_lO_tQhd8hv"},"source":["### RAG Questions\n"]},{"cell_type":"markdown","metadata":{"id":"IxaiUb_1d8hv"},"source":["We now have Marengo embeddings and Pegasus video IDs upload to Weaviate.\n","\n","We can assess the performance of running queries on the clips and the full video in terms of answer accuracy and speed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hs2H_4J9d8hv"},"outputs":[],"source":["video_questions = [\n","    \"In the American Football Video, what are the teams playing?\",\n","    \"Which arm does Eli Manning throw the ball with?\",\n","    \"In the tennis match video, who is playing?\",\n","    \"What foot does Messi shoot at the goal with?\",\n","    \"When Does Keri Strug hurt her foot?\"\n","]"]},{"cell_type":"markdown","metadata":{"id":"Yj5HLBRLd8hw"},"source":["### Multi Video RAG with Pegasus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GCX_9YCd8hw"},"outputs":[],"source":["from weaviate.classes.query import MetadataQuery, Filter\n","import time\n","\n","pegasus_full_video_answers = []\n","\n","start_time = time.time()\n","\n","for question in video_questions:\n","\n","    embedding = twelve_labs_client.embed.create(\n","        model_name=\"Marengo-retrieval-2.7\",\n","        text=question,\n","        text_truncate=\"start\",\n","    )\n","\n","    query_vector = embedding.text_embedding.segments[0].embeddings_float\n","\n","    response = collection.query.near_vector(\n","        near_vector=query_vector,\n","        limit=1,\n","        return_metadata=MetadataQuery(distance=True),\n","        filters=(Filter.by_property(\"type\").equal(\"video\"))\n","    )\n","\n","    selected_video_name = response.objects[0].properties[\"video_file\"]\n","    selected_video_id = response.objects[0].properties[\"pegasus_video_id\"]\n","\n","    res = twelve_labs_client.generate.text(\n","        video_id=selected_video_id,\n","        prompt=question\n","    )\n","\n","    pegasus_full_video_answers.append([question,selected_video_name,res.data])\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Execution time: {int(execution_time)} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDQDxID1d8hw"},"outputs":[],"source":["pegasus_clip_video_answers = []\n","\n","start_time = time.time()\n","\n","for question in video_questions:\n","    embedding = twelve_labs_client.embed.create(\n","        model_name=\"Marengo-retrieval-2.7\",\n","        text=question,\n","        text_truncate=\"start\",\n","    )\n","\n","    query_vector = embedding.text_embedding.segments[0].embeddings_float\n","\n","    response = collection.query.near_vector(\n","        near_vector=query_vector,\n","        limit=1,\n","        return_metadata=MetadataQuery(distance=True),\n","        filters=(Filter.by_property(\"type\").equal(\"clip\"))\n","    )\n","\n","    selected_video_name = response.objects[0].properties[\"video_file\"]\n","    selected_video_id = response.objects[0].properties[\"pegasus_video_id\"]\n","\n","    res = twelve_labs_client.generate.text(\n","        video_id=selected_video_id,\n","        prompt=question\n","    )\n","\n","    pegasus_clip_video_answers.append([question,selected_video_name,res.data])\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Execution time: {int(execution_time)} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cl2gKorkd8hw"},"outputs":[],"source":["for clip_answer, full_answer in zip(pegasus_clip_video_answers, pegasus_full_video_answers):\n","\n","    print(\"question\",clip_answer[0])\n","    print(\"clip:  \",clip_answer[2])\n","    print(\"full:  \",full_answer[2])\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"KskD1Ja7d8hx"},"source":["### Multi Video RAG with LLaVa-NeXT-Video\n","Now we can run our model on the full video, which outputs some more interesting answers"]},{"cell_type":"markdown","metadata":{"id":"OJX4sJN2d8hx"},"source":["First we'll sample the rest of our video segments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Cnyy9QUd8hx"},"outputs":[],"source":["for video_file in os.listdir(video_segments_dir):\n","    print(video_file)\n","    sampled_video = sample_video(video_segments_dir + video_file,num_samples=40)\n","    sampled_video_files[video_file] = sampled_video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7n6hydryd8hx"},"outputs":[],"source":["llava_full_video_answers = []\n","\n","start_time = time.time()\n","\n","for question in video_questions:\n","    embedding = twelve_labs_client.embed.create(\n","        model_name=\"Marengo-retrieval-2.7\",\n","        text=question,\n","        text_truncate=\"start\"\n","    )\n","\n","    query_vector = embedding.text_embedding.segments[0].embeddings_float\n","\n","    response = collection.query.near_vector(\n","        near_vector=query_vector,\n","        limit=1,\n","        return_metadata=MetadataQuery(distance=True),\n","        filters=(Filter.by_property(\"type\").equal(\"video\"))\n","    )\n","\n","    selected_video_file = response.objects[0].properties[\"video_file\"]\n","    selected_video_id = response.objects[0].properties[\"pegasus_video_id\"]\n","\n","    sampled_video = sampled_video_files[selected_video_file]\n","    generated_text = query_llava_next(question,llava_next_model,llava_next_processor,sampled_video)\n","\n","    llava_full_video_answers.append([question,selected_video_name,generated_text])\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Execution time: {int(execution_time)} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SubqHdWd8hy"},"outputs":[],"source":["from weaviate.classes.query import MetadataQuery\n","\n","import time\n","\n","llava_clip_video_answers = []\n","\n","start_time = time.time()\n","\n","for question in video_questions:\n","    embedding = twelve_labs_client.embed.create(\n","        model_name=\"Marengo-retrieval-2.7\",\n","        text=question,\n","        text_truncate=\"start\"\n","    )\n","\n","    query_vector = embedding.text_embedding.segments[0].embeddings_float\n","\n","    response = collection.query.near_vector(\n","        near_vector=query_vector,\n","        limit=1,\n","        return_metadata=MetadataQuery(distance=True),\n","        filters=(Filter.by_property(\"type\").equal(\"clip\"))\n","    )\n","\n","    selected_video_file = response.objects[0].properties[\"video_file\"]\n","    selected_video_id = response.objects[0].properties[\"pegasus_video_id\"]\n","\n","    sampled_video = sampled_video_files[selected_video_file]\n","    generated_text = query_llava_next(question,llava_next_model,llava_next_processor,sampled_video)\n","\n","    llava_clip_video_answers.append([question,selected_video_name,generated_text])\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Execution time: {int(execution_time)} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mvik6RYcd8hy"},"outputs":[],"source":["for clip_answer, full_answer in zip(llava_clip_video_answers, llava_full_video_answers):\n","\n","    print(\"question\",clip_answer[0])\n","    print(\"clip:  \",clip_answer[2])\n","    print(\"full:  \",full_answer[2])\n","    print(\"\\n\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}