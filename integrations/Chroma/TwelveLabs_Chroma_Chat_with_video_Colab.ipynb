{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSVBp7uxVXvP"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook we will walk you through examples of how to use Twelve Labs Marengo embedding model to create video embeddings, use Chroma to store and query those embeddings, and use Twelve Labs Pegasus model to chat with the returned videos. We will also compare Pegasus with a leading open source model.\n",
    "\n",
    "We will:\n",
    "1. Create Video Embeddings Using the Twelve Labs Marengo Engine\n",
    "2. Store Video Embeddings in a Chroma Database\n",
    "3. Query Embeddings in our Chroma Database to Find Relevant Video Segments\n",
    "4. Use Twelve Labs Pegasus to Chat with the Returned Video Segment\n",
    "5. Use an Open Source Model to Chat with the Returned Video Segment\n",
    "6. Compare Pegasus to the Open Source model\n",
    "7. Use Chroma and Twelve Labs Embeddings to Search Multiple Videos\n",
    "8. Use Pegasus to Chat with a Full Video\n",
    "9. Use an Open Source Model to Chat with a Full Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5XdWL4-VXvR"
   },
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6871,
     "status": "ok",
     "timestamp": 1746320078165,
     "user": {
      "displayName": "Keith Fearon",
      "userId": "07808994618986968853"
     },
     "user_tz": 420
    },
    "id": "Zub7_pYwF24i",
    "outputId": "38c180dd-117f-4a29-e7b6-cf02ab01f494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twelvelabs in /usr/local/lib/python3.11/dist-packages (0.4.7)\n",
      "Requirement already satisfied: pydantic>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from twelvelabs) (2.11.3)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from twelvelabs) (0.28.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->twelvelabs) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->twelvelabs) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->twelvelabs) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->twelvelabs) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->twelvelabs) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->twelvelabs) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->twelvelabs) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->twelvelabs) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->twelvelabs) (0.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->twelvelabs) (1.3.1)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.7)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.17)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "#Install Twelve Labs and Chroma libraries\n",
    "!pip install --upgrade twelvelabs\n",
    "!pip install --upgrade chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVTpaHfJF24j"
   },
   "outputs": [],
   "source": [
    "#Install libraries for use with the open source model\n",
    "!pip install protobuf==3.20.3\n",
    "!pip install --upgrade -q accelerate bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPuAP_0ZF24j"
   },
   "outputs": [],
   "source": [
    "# #Extra Things to install if you're not on colab\n",
    "# !python -m pip install pillow\n",
    "# !python -m pip install sentencepiece\n",
    "# !python -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv5cDuwPwunQ"
   },
   "source": [
    "## Preparing the Video Data\n",
    "\n",
    "### Using our video Data\n",
    "\n",
    "This demo uses video data from a Twelve Labs google drive folder. To use it, you'll need to link the folder to your google drive, and then mount your google drive to this colab.\n",
    "\n",
    "### Linking the folder to our Google Drive:\n",
    "Anyone can access the folder with this link: https://drive.google.com/drive/folders/1k6FmkVglFsdtJG4MTIK-2dk1Dk9gTPtu?usp=share_link\n",
    "\n",
    "To Link this to the correct spot in _your_ google drive:\n",
    "1. Go to \"Shared with me\" in Google Drive.\n",
    "2. Locate the shared folder you want to access.\n",
    "3. Select \"Organize\" -> \"Add Shortcut\"\n",
    "4. Choose \"My Drive\" as the destination and click \"Add\".\n",
    "\n",
    "Now this folder should be accessible at `/content/drive/MyDrive/TwelveLabs-Chroma`\n",
    "\n",
    "### Mounting Drive\n",
    "The cell below will mount your Drive, which we can then use to load the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 862,
     "status": "ok",
     "timestamp": 1746320085304,
     "user": {
      "displayName": "Keith Fearon",
      "userId": "07808994618986968853"
     },
     "user_tz": 420
    },
    "id": "iqebdOgrGLJE",
    "outputId": "99be3ba6-90ca-4aac-ccd8-ed3f680cfbda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj5Ikn-zTWu1"
   },
   "source": [
    "### Set Video Path\n",
    "\n",
    "Here we set the path for the videos we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1746320689613,
     "user": {
      "displayName": "Keith Fearon",
      "userId": "07808994618986968853"
     },
     "user_tz": 420
    },
    "id": "Ggx2TZt9VXvV"
   },
   "outputs": [],
   "source": [
    "video_folder_path = \"/content/drive/MyDrive/its_nathalievictoria\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQ0flQmWF24k"
   },
   "source": [
    "### Upscale Video Resolution\n",
    "Some of our videos are too low resolution to use in the embedding engine, so we will double their resolution with `upscale_video`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdGA2IMFF24k"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def upscale_video(input_file, output_path, target_width=854, target_height=480):\n",
    "\n",
    "    output_file = os.path.join(output_path, os.path.basename(input_file))\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Skipping {input_file} as {output_file} already exists.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Upscale a video to the target width and height using FFmpeg.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input video file.\n",
    "        output_file (str): Path to save the upscaled video.\n",
    "        target_width (int): Desired output width. Default is 854.\n",
    "        target_height (int): Desired output height. Default is 480.\n",
    "    \"\"\"\n",
    "    # FFmpeg command to upscale the video\n",
    "    ffmpeg_command = [\n",
    "        'ffmpeg',\n",
    "        '-i', input_file,                              # Input file\n",
    "        '-vf', f'scale={target_width}:{target_height}', # Scale filter with target dimensions\n",
    "        '-c:a', 'copy',                                # Copy audio stream without re-encoding\n",
    "        output_file,                                    # Output file\n",
    "        \"-y\"\n",
    "    ]\n",
    "\n",
    "    # Run the FFmpeg command\n",
    "    subprocess.run(ffmpeg_command)\n",
    "\n",
    "    print(f\"Upscaled video saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Z6fCDrjVXvW"
   },
   "outputs": [],
   "source": [
    "upscaled_video_dir = video_folder_path + \"upscaled_videos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dS8Gs41KVXvW"
   },
   "outputs": [],
   "source": [
    "#Upscale all .mp4 videos\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(upscaled_video_dir):\n",
    "    os.makedirs(upscaled_video_dir)\n",
    "\n",
    "# Iterate over all files in the raw video directory\n",
    "for filename in os.listdir(video_folder_path):\n",
    "    # Check if the file is a video file\n",
    "    input_filepath = os.path.join(video_folder_path, filename)\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        upscale_video(input_filepath, upscaled_video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KnYlHMcVXvX"
   },
   "source": [
    "## Create Video Embeddings Using the Twelve Labs Marengo Engine\n",
    "\n",
    "Here we will use the Twelve Labs Marengo Engine to create embeddings for our video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1746320818538,
     "user": {
      "displayName": "Keith Fearon",
      "userId": "07808994618986968853"
     },
     "user_tz": 420
    },
    "id": "nNpn8UKuF24k"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "TL_API_KEY=userdata.get('TL_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1746320823352,
     "user": {
      "displayName": "Keith Fearon",
      "userId": "07808994618986968853"
     },
     "user_tz": 420
    },
    "id": "KoPmnY7ytmMV"
   },
   "outputs": [],
   "source": [
    "from twelvelabs import TwelveLabs\n",
    "from twelvelabs.models.embed import EmbeddingsTask\n",
    "\n",
    "# Initialize the Twelve Labs client\n",
    "twelvelabs_client = TwelveLabs(api_key=TL_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1746320825274,
     "user": {
      "displayName": "Keith Fearon",
      "userId": "07808994618986968853"
     },
     "user_tz": 420
    },
    "id": "Tm_UXJtlVXvY"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51mx5aqgVXvZ"
   },
   "source": [
    "### Create Video Embeddings and Format for Chroma\n",
    "Here we create video embeddings using Marengo and format for Chroma. To upload data to chroma you need three separate lists for all the data that you want to upload: `embeddings`, `metadatas`, and `ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmvkfiNLF24l"
   },
   "outputs": [],
   "source": [
    "def on_task_update(task: EmbeddingsTask):\n",
    "    print(f\"  Status={task.status}\")\n",
    "\n",
    "# Create video embeddings and format for Chroma\n",
    "def create_video_embeddings(client,video_file,segment_length,task_id=None):\n",
    "\n",
    "    #upload video to twelve labs if it does not already exist\n",
    "    video_name = os.path.basename(video_file)\n",
    "\n",
    "    if task_id == None or task_id == \"\":\n",
    "        task = client.embed.task.create(\n",
    "            engine_name=\"Marengo-retrieval-2.7\",\n",
    "            video_file=video_file,\n",
    "            video_clip_length=segment_length\n",
    "        )\n",
    "        print(\n",
    "            f\"Created task: id={task.id} engine_name={task.engine_name} status={task.status}\"\n",
    "        )\n",
    "\n",
    "        status = task.wait_for_done(\n",
    "            sleep_interval=2,\n",
    "            callback=on_task_update\n",
    "        )\n",
    "\n",
    "        print(f\"Embedding done: {status}\")\n",
    "\n",
    "        task_id = task.id\n",
    "\n",
    "    #fetch embeddings\n",
    "    task = client.embed.task.retrieve(task_id)\n",
    "\n",
    "    #format for chroma\n",
    "    embeddings = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    print(\"embeddings\",task.video_embeddings)\n",
    "\n",
    "    if task.video_embeddings is not None:\n",
    "        for v in task.video_embeddings:\n",
    "\n",
    "            metadata = {\n",
    "                \"embedding_scope\":v.embedding_scope,\n",
    "                \"start_offset_sec\":v.start_offset_sec,\n",
    "                \"end_offset_sec\":v.end_offset_sec,\n",
    "                \"video_file\":video_file,\n",
    "                \"video_name\":video_name,\n",
    "                \"task_id\":task.id,\n",
    "                \"video_segment_number\":idx\n",
    "            }\n",
    "\n",
    "\n",
    "            embedding = v.values\n",
    "            id = task.id + \"_\" + str(idx)\n",
    "\n",
    "            metadatas.append(metadata)\n",
    "            embeddings.append(embedding)\n",
    "            ids.append(id)\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    return (ids,metadatas,embeddings,task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XevyVAd_VXvZ"
   },
   "outputs": [],
   "source": [
    "#set the segment duration and the video we will be working with\n",
    "segment_duration = 6\n",
    "current_video_path = upscaled_video_dir + \"How To Make Birria Tacos [4nIFJFgH99w].mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0bxliY1VXva"
   },
   "outputs": [],
   "source": [
    "#Get embeddings to upload to Chroma\n",
    "\n",
    "#Set task_id if you already have one, otherwise set to empty string\n",
    "task_id = \"\"\n",
    "ids, metadatas, embeddings, task_id = create_video_embeddings(twelvelabs_client,current_video_path,segment_duration,task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-lLn4ewF24m"
   },
   "source": [
    "## Store Video Embeddings in a Chroma Database\n",
    "\n",
    "Now that we have our records and vectors in an easy format, we can simply add them to a new collection in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rW1D-pesVXva"
   },
   "outputs": [],
   "source": [
    "#Fetch or create a Chroma Collection\n",
    "chroma_collection_name = \"video_embeddings\"\n",
    "collection = chroma_client.get_or_create_collection(chroma_collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz6xh3r6F24m"
   },
   "outputs": [],
   "source": [
    "#Add embeddings and metadata to our collection\n",
    "collection.add(\n",
    "    metadatas = metadatas,\n",
    "    embeddings = embeddings,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxpfgbo4VXvb"
   },
   "source": [
    "## Query Embeddings in our Chroma Database to Find Relevant Video Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmixEdCdF24m"
   },
   "source": [
    "### Testing the Vector Search\n",
    "Now that we have everything in the collection, we can test and see that the embeddings query works. We will search by the first returned embedding and expect it's distance to itself is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX3EsfkLVXvc"
   },
   "outputs": [],
   "source": [
    "#use first embedding as a test search\n",
    "test_segment_embeddings = embeddings[0]\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[test_segment_embeddings],\n",
    "    n_results=4\n",
    ")\n",
    "\n",
    "print(\"search embeddings for:\",ids[0])\n",
    "print(\"found:\", results[\"ids\"][0][0])\n",
    "print(\"distance:\",results[\"distances\"][0][0])\n",
    "\n",
    "#assert that the first video's text embedding is distance 0 from itself\n",
    "assert results[\"ids\"][0][0] == ids[0]\n",
    "assert results[\"distances\"][0][0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T33L5j-xF24m"
   },
   "source": [
    "### Querying our Vector Database\n",
    "With our queries, we can embed them, and then perform a vector search in our database. While these embeddings are related to segments of an overall video, they include the full video name associated with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VIpjCiaF24m"
   },
   "outputs": [],
   "source": [
    "query = \"What are the ingredients for birra tacos?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stClvm0AF24m"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def query_chroma(collection,query,n_results=1):\n",
    "    #Create embedding for query\n",
    "    embedding = twelvelabs_client.embed.create(\n",
    "        engine_name=\"Marengo-retrieval-2.6\",\n",
    "        text=query,\n",
    "        text_truncate=\"start\",\n",
    "    )\n",
    "\n",
    "    query_embeddings = embedding.text_embedding.float\n",
    "\n",
    "    #Search Chroma database with query embedding\n",
    "\n",
    "    response = collection.query(\n",
    "        query_embeddings=query_embeddings,\n",
    "        n_results=n_results,\n",
    "        # return_metadata=MetadataQuery(distance=True),\n",
    "    )\n",
    "\n",
    "\n",
    "    return response\n",
    "\n",
    "response = query_chroma(collection,query)\n",
    "\n",
    "# Print the properties and distance of the most similar object\n",
    "print(response[\"ids\"][0][0])\n",
    "print(response[\"distances\"][0][0])\n",
    "print(response[\"metadatas\"][0][0])\n",
    "\n",
    "# Get the path for the found video segment for the next step\n",
    "found_video_metadata = response[\"metadatas\"][0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYyMr4qpVXve"
   },
   "source": [
    "## Splitting Videos into Segments\n",
    "We will now split the videos into 6 second segments.\n",
    "This will match the segment duration of our embeddings allowing us to submit _only_ this video chunk to our model for a RAG use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYu8KdUKVXve"
   },
   "outputs": [],
   "source": [
    "split_video_dir = video_folder_path + \"split_videos/\"\n",
    "\n",
    "def split_video(input_path, output_dir, segment_duration=6):\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    filename = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    filetype = os.path.splitext(os.path.basename(input_path))[1]\n",
    "\n",
    "    # Split video into segments\n",
    "    ffmpeg_command = [\n",
    "        'ffmpeg',\n",
    "        '-i', input_path,             # Input video file\n",
    "        '-c', 'copy',                  # Copy both video and audio codecs\n",
    "        '-f', 'segment',               # Segment mode\n",
    "        '-segment_time', str(segment_duration),  # Segment length\n",
    "        '-reset_timestamps', '1',      # Reset timestamps for each segment\n",
    "        output_dir + filename + '_%03d' + filetype  # Output filename pattern (e.g., output_001.mp4)\n",
    "    ]\n",
    "\n",
    "    # Run the command\n",
    "    subprocess.run(ffmpeg_command)\n",
    "\n",
    "    print(\"Video split into 6-second segments successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTkCqrSfVXvf"
   },
   "outputs": [],
   "source": [
    "#Split the video into segments\n",
    "split_video(input_path=current_video_path, output_dir=split_video_dir, segment_duration=segment_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2V0rq_FwRVv"
   },
   "source": [
    "## Use Twelve Labs Pegasus to Chat with the Returned Video Segment\n",
    "These next few cells will show us how simple it is to use Pegasus to chat with a video -- Everything comes ready out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKZR-7qLwT9t"
   },
   "source": [
    "### Uploading Video Segment to Pegasus\n",
    "First we will create an index for our video uploads and the Pegasus Engine, then upload them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sL22aYtlwXNi"
   },
   "outputs": [],
   "source": [
    "#create or retrieve pegasus index\n",
    "engines = [\n",
    "        {\n",
    "            \"name\": \"pegasus1.1\",\n",
    "            \"options\": [\"visual\", \"conversation\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "index_name = \"cooking_video_index\"\n",
    "indices_list = twelvelabs_client.index.list(name=index_name)\n",
    "\n",
    "if len(indices_list) == 0:\n",
    "    index = twelvelabs_client.index.create(\n",
    "        name=index_name,\n",
    "        engines=engines,\n",
    "\n",
    "    )\n",
    "    print(f\"A new index has been created: id={index.id} name={index.name} engines={index.engines}\")\n",
    "else:\n",
    "    index = indices_list[0]\n",
    "    print(f\"Index already exists: id={index.id} name={index.name} engines={index.engines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIL3WsKKVXvh"
   },
   "source": [
    "### Get Video Segment File Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83jmEUonVXvh"
   },
   "outputs": [],
   "source": [
    "#Get video segment filename\n",
    "found_video_segment_number = int(found_video_metadata[\"video_segment_number\"])\n",
    "found_video_file = found_video_metadata[\"video_file\"]\n",
    "found_video_filename = os.path.splitext(os.path.basename(found_video_file))[0]\n",
    "found_video_filetype = os.path.splitext(os.path.basename(found_video_file))[1]\n",
    "found_video_segment_filename = found_video_filename + f\"_{found_video_segment_number:03d}\"\n",
    "\n",
    "found_video_segment_path = split_video_dir + found_video_segment_filename + found_video_filetype\n",
    "print(found_video_segment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn4mjCEcVXvi"
   },
   "source": [
    "### Upload Video to Pegasus and Get Video Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wfr2vtwxwZ8-"
   },
   "outputs": [],
   "source": [
    "def upload_video_to_twelve_labs(index,video_path):\n",
    "\n",
    "    #upload our video to our twelve labs index\n",
    "    task = twelvelabs_client.task.create(\n",
    "        index_id=index.id,\n",
    "        file = video_path\n",
    "    )\n",
    "    print(f\"Task created: id={task.id} status={task.status}\")\n",
    "\n",
    "    task.wait_for_done(sleep_interval=5, callback=on_task_update)\n",
    "\n",
    "    if task.status != \"ready\":\n",
    "      raise RuntimeError(f\"Indexing failed with status {task.status}\")\n",
    "    print(f\"The unique identifer of your video is {task.video_id}.\")\n",
    "\n",
    "    #return the video id\n",
    "    return task.video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEjTBBtGVXvj"
   },
   "outputs": [],
   "source": [
    "#Set video_id if you already have one, otherwise set to empty string\n",
    "video_id = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgkR5G6vVXvj"
   },
   "outputs": [],
   "source": [
    "#Upload video to get video id to chat with in Pegasus\n",
    "if video_id == \"\":\n",
    "    video_id = upload_video_to_twelve_labs(index,found_video_segment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9TPHDkcwlQv"
   },
   "source": [
    "### Calling Pegasus\n",
    "Here we query the video segmement with the query we used to find it. Because TwelveLabs handles all of the boilerplate behind the scenes, we can call our model with a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tSiXHTwwm6o"
   },
   "outputs": [],
   "source": [
    "#chat with the video segment using Pegasus with the query we used to find it\n",
    "res = twelvelabs_client.generate.text(\n",
    "  video_id=video_id,\n",
    "  prompt=query\n",
    ")\n",
    "segment_answer = res.data\n",
    "print(f\"query: {query}\")\n",
    "print(f\"{segment_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W91LXi5jwv0w"
   },
   "source": [
    "## Use an Open Source Model to Chat with the Returned Video Segment\n",
    "First, we need to sample the videos ourselves for the model to consume. We'll modify the [LLaVa-NeXT-Video Sampling code](https://colab.research.google.com/drive/1CZggLHrjxMReG-FNOmqSOdi4z7NPq6SO?usp=sharing#scrollTo=hqpPqDKuQUTq) to get a uniform sample of 8 frames for each video.\n",
    "\n",
    "And we can do this for all of the video segments in our folder.\n",
    "\n",
    "`read_video_pyav` comes directly from the [LLaVa-NeXT-Video collab notebook](https://colab.research.google.com/drive/1CZggLHrjxMReG-FNOmqSOdi4z7NPq6SO?usp=sharing#scrollTo=hqpPqDKuQUTq) and it formats videos in the correct numpy representation for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEBxXIEcF24l"
   },
   "outputs": [],
   "source": [
    "import av\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "\n",
    "    Args:\n",
    "        container (av.container.input.InputContainer): PyAV container.\n",
    "        indices (List[int]): List of frame indices to decode.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_video(video_path, num_samples=8):\n",
    "    container = av.open(video_path)\n",
    "\n",
    "    # sample uniformly num_samples frames from the video\n",
    "    total_frames = container.streams.video[0].frames\n",
    "    indices = np.arange(0, total_frames, total_frames / num_samples).astype(int)\n",
    "\n",
    "    sampled_frames = read_video_pyav(container, indices)\n",
    "\n",
    "    return sampled_frames\n",
    "\n",
    "def process_videos_in_folder(folder_path):\n",
    "    sample_info = {}\n",
    "\n",
    "    # Supported video file extensions\n",
    "    video_extensions = ('.mp4', '.avi', '.mov', '.mkv')\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        simple_video_name = os.path.splitext(os.path.basename(filename))[0]\n",
    "        if filename.lower().endswith(video_extensions):\n",
    "            video_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                sampled_clip = sample_video(video_path)\n",
    "                sample_info[simple_video_name] = {\"sampled_video\": sampled_clip, \"video_path\" : video_path}\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "    return sample_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6S9AqkmvF24l"
   },
   "outputs": [],
   "source": [
    "sampled_video_info = process_videos_in_folder(split_video_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ETEjQNcw6aR"
   },
   "outputs": [],
   "source": [
    "# Get video segment found in our Chroma query\n",
    "video_segment = sampled_video_info[found_video_segment_filename]['sampled_video']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygJC80XAF24m"
   },
   "source": [
    "### Setting up our Model\n",
    "We'll set up our model in 4-bit quantization to speed up inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5k54iYFrF24m"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "op93Ip5_F24m"
   },
   "outputs": [],
   "source": [
    "#To use later to play the videos in the notebook itself\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "# pick one at random just to see\n",
    "video = sampled_video_info[list(sampled_video_info.keys())[0]]['sampled_video']\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "                               interval=100)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOlLB3coF24m"
   },
   "source": [
    "### Running the Model\n",
    "Now that we have our query and the relevant video, we can feed them into the model to get an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fyp6o0QkF24m"
   },
   "outputs": [],
   "source": [
    "# Each \"content\" is a list of dicts and you can add image/video/text modalities\n",
    "conversation = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": query},\n",
    "              {\"type\": \"video\"},\n",
    "              ],\n",
    "      },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "prompt_len = len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f6FlFmsF24m"
   },
   "outputs": [],
   "source": [
    "inputs = processor([prompt], videos=[video_segment], padding=True, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUZQXQGlF24m"
   },
   "outputs": [],
   "source": [
    "generate_kwargs = {\"max_new_tokens\": 100, \"do_sample\": True, \"top_p\": 0.9}\n",
    "\n",
    "output = model.generate(**inputs, **generate_kwargs)\n",
    "open_source_segment_generated_text = processor.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_nQvvOGF24m"
   },
   "outputs": [],
   "source": [
    "print(open_source_segment_generated_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7SlT-igVXvn"
   },
   "source": [
    "## Compare Pegasus to the Open Source model\n",
    "\n",
    "As we can see Pegasus does a better job at answering our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdKaAJt9VXvn"
   },
   "outputs": [],
   "source": [
    "print(f\"query {query}\")\n",
    "print(\"pegasus answer\")\n",
    "print(segment_answer)\n",
    "print(\"open source answer\")\n",
    "print(open_source_segment_generated_text[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHBCrPyIF24n"
   },
   "source": [
    "## Using Chroma and Twelve Labs Embeddings to Search Multiple Videos\n",
    "With a series of videos, we can better show the value of Semantic Search and RAG by giving our model the full video from a query, out of a potential large set of videos. The Semantic Search allows us to find the specific video that we need to answer the query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7NVXI7MF24n"
   },
   "source": [
    "### Embedding our Video Database:\n",
    "First we will embed all of our videos and store those embeddings in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7bGUbyxVXvn"
   },
   "outputs": [],
   "source": [
    "#embed and store task ID's for all videos\n",
    "chroma_collection_name = \"video_embeddings\"\n",
    "collection = chroma_client.get_or_create_collection(chroma_collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HStAVyOBVXvo"
   },
   "outputs": [],
   "source": [
    "#store twelve labs task ids for each video\n",
    "task_ids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcD_RNHSF24n"
   },
   "outputs": [],
   "source": [
    "#get embeddings and metadata for each video\n",
    "#store task ids so we don't upload videos multiple times\n",
    "for filename in os.listdir(upscaled_video_dir):\n",
    "\n",
    "    if filename.endswith(\".mp4\"):\n",
    "\n",
    "        if (filename in task_ids.keys()):\n",
    "            task_id = task_ids[filename]\n",
    "        else:\n",
    "            task_id = None\n",
    "\n",
    "        file_path = os.path.join(upscaled_video_dir, filename)\n",
    "\n",
    "        ids, metadatas, embeddings, task_id = create_video_embeddings(twelvelabs_client,file_path,segment_duration,task_id)\n",
    "\n",
    "        task_ids[filename] = task_id\n",
    "\n",
    "        collection.add(\n",
    "            metadatas = metadatas,\n",
    "            embeddings = embeddings,\n",
    "            ids=ids\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4rBkyF9F24n"
   },
   "outputs": [],
   "source": [
    "print(task_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jCJECjLF24q"
   },
   "source": [
    "### Querying our Database\n",
    "Here we query the database for full videos to chat with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNqwVFNnF24q"
   },
   "outputs": [],
   "source": [
    "response = query_chroma(collection,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUJbeXxbF24q"
   },
   "outputs": [],
   "source": [
    "found_full_video_name = response[\"metadatas\"][0][0][\"video_name\"]\n",
    "print(found_full_video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L7apQM31PI5"
   },
   "source": [
    "## Use Pegasus to Chat with a Full Video\n",
    "We already have an index created, so we just need to upload the videos to this index then call Pegasus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFKt-5o3VXvp"
   },
   "outputs": [],
   "source": [
    "#store pegasus video ids so that we don't upload videos multiple times\n",
    "pegasus_video_ids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsUDTCPa1SVL"
   },
   "outputs": [],
   "source": [
    "\n",
    "for upscaled_video in os.listdir(upscaled_video_dir):\n",
    "    upscaled_video_path = os.path.join(upscaled_video_dir, upscaled_video)\n",
    "    print(upscaled_video_path)\n",
    "    if upscaled_video not in pegasus_video_ids:\n",
    "        video_id = upload_video_to_twelve_labs(index,upscaled_video_path)\n",
    "        pegasus_video_ids[upscaled_video] = video_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RA35enc4VXvq"
   },
   "outputs": [],
   "source": [
    "print(pegasus_video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANrICSPX1ZjY"
   },
   "source": [
    "### Calling Pegasus to Chat with Full Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHRq_4GP1drO"
   },
   "outputs": [],
   "source": [
    "video_id = pegasus_video_ids[found_full_video_name]\n",
    "print(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NY_tRb8qVXvq"
   },
   "outputs": [],
   "source": [
    "res = twelvelabs_client.generate.text(\n",
    "  video_id=video_id,\n",
    "  prompt=query\n",
    ")\n",
    "full_video_answer = res.data\n",
    "print(f\"query {query}\")\n",
    "print(f\"{full_video_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vALwAv-VXvq"
   },
   "source": [
    "### Compare full video answer to segment answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0z0-9VvYVXvq"
   },
   "outputs": [],
   "source": [
    "print(f\"segment answer: \\n{segment_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxBoaYhA1n0k"
   },
   "source": [
    "## Use an Open Source Model to Chat with a Full Video\n",
    "After we sample all of the videos again, we can run our model on the full video, which outputs some more interesting answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuDHLWkAF24q"
   },
   "outputs": [],
   "source": [
    "#sample all of the videos:\n",
    "sampled_database_video_info = process_videos_in_folder(upscaled_video_dir)\n",
    "print(sampled_database_video_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64AaUykbF24q"
   },
   "outputs": [],
   "source": [
    "# Each \"content\" is a list of dicts and you can add image/video/text modalities\n",
    "conversation = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": query},\n",
    "              {\"type\": \"video\"},\n",
    "              ],\n",
    "      },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "prompt_len = len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zD3JccOeF24q"
   },
   "outputs": [],
   "source": [
    "inputs = processor([prompt], videos=[video_segment], padding=True, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYhdWVMIF24q"
   },
   "outputs": [],
   "source": [
    "generate_kwargs = {\"max_new_tokens\": 100, \"do_sample\": True, \"top_p\": 0.9}\n",
    "\n",
    "output = model.generate(**inputs, **generate_kwargs)\n",
    "generated_text = processor.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlGzRAj7F24q"
   },
   "outputs": [],
   "source": [
    "print(generated_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNjMM0r9VXvs"
   },
   "source": [
    "### Compare Result to Pegasus Answer\n",
    "As we can see the open source model cannot give us an answer when chatting with the entire video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3xPpMs-VXvs"
   },
   "outputs": [],
   "source": [
    "print(f\"Pegasus answer: \\n{full_video_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lypFzY-yVXvs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hf-env",
   "language": "python",
   "name": "hf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
